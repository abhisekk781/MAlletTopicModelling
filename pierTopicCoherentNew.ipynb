{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda install -c conda-forge sparse_dot_topn\n",
    "#conda install numba\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from sparse_dot_topn import awesome_cossim_topn\n",
    "from scipy.sparse import csr_matrix\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(from_vector: np.ndarray,\n",
    "                      to_vector: np.ndarray,\n",
    "                      from_list: List[str],\n",
    "                      to_list: List[str],\n",
    "                      to_key_list: List[str],\n",
    "                      nbest,\n",
    "                      min_similarity: float = 0) -> pd.DataFrame:\n",
    "    \n",
    "    if nbest != None:\n",
    "        if int(nbest) >  len(to_list):\n",
    "            raise ValueError('best choice must be less than to_list')\n",
    "    else:\n",
    "        nbest = int(1)\n",
    "\n",
    "    if isinstance(to_vector, np.ndarray):\n",
    "        to_vector = csr_matrix(to_vector)\n",
    "    if isinstance(from_vector, np.ndarray):\n",
    "        from_vector = csr_matrix(from_vector)\n",
    "\n",
    "    # There is a bug with awesome_cossim_topn that when to_vector and from_vector\n",
    "    # have the same shape, setting topn to 1 does not work. Apparently, you need\n",
    "    # to it at least to 2 for it to work\n",
    "\n",
    "    if int(nbest) <= 1:\n",
    "        similarity_matrix = awesome_cossim_topn(from_vector, to_vector.T, 2, min_similarity)\n",
    "    elif int(nbest) > 1:\n",
    "        similarity_matrix = awesome_cossim_topn(from_vector, to_vector.T, nbest, min_similarity)\n",
    "\n",
    "    if from_list == to_list:\n",
    "        similarity_matrix = similarity_matrix.tolil()\n",
    "        similarity_matrix.setdiag(0.)\n",
    "        similarity_matrix = similarity_matrix.tocsr()\n",
    "\n",
    "    if int(nbest) <= 1:\n",
    "        indices = np.array(similarity_matrix.argmax(axis=1).T).flatten()\n",
    "        similarity = similarity_matrix.max(axis=1).toarray().T.flatten()\n",
    "    elif int(nbest) > 1:\n",
    "        similarity = np.flip(np.take_along_axis(similarity_matrix.toarray(), np.argsort(similarity_matrix.toarray(), axis =1), axis=1) [:,-int(nbest):], axis = 1)\n",
    "        indices = np.flip(np.argsort(np.array(similarity_matrix.toarray()), axis =1)[:,-int(nbest):], axis = 1)\n",
    "            \n",
    "    \n",
    "    if int(nbest) <= 1:\n",
    "        matches = [to_list[idx] for idx in indices.flatten()]\n",
    "        key_matches = [to_key_list[idx] for idx in indices.flatten()]\n",
    "        matches = pd.DataFrame(np.vstack((from_list, matches, key_matches, similarity)).T, columns=[\"From\", \"To\", \"Key\", \"Similarity\"])\n",
    "        matches.Similarity = matches.Similarity.astype(float)\n",
    "        matches.loc[matches.Similarity < 0.001, \"To\"] = None\n",
    "        matches.loc[matches.Similarity < 0.001, \"Key\"] = None\n",
    "    else:\n",
    "        matches = [np.array([to_list[idx] for idx in l]) for l in indices] ##In progress\n",
    "        key_matches = [np.array([to_key_list[idx] for idx in l]) for l in indices] ##In progress\n",
    "        column = []\n",
    "        column.append(\"To\")\n",
    "        for i in range(int(nbest) - 1):\n",
    "            column.append(\"BestMatch\" + \"__\" + str(i+1))\n",
    "        column.append(\"Key\")\n",
    "        for j in range(int(nbest) - 1):\n",
    "            column.append(\"Key\" + \"__\" + str(j+1))\n",
    "        column.append(\"Similarity\")\n",
    "        for j in range(int(nbest) - 1):\n",
    "            column.append(\"Similarity\" + \"__\" + str(j+1))\n",
    "            \n",
    "        matches = pd.concat([pd.DataFrame({'From' : from_list}), pd.DataFrame(np.hstack((matches, key_matches, similarity)), columns= column)], axis =1)\n",
    "        matches.Similarity = matches.Similarity.astype(float)\n",
    "        matches.loc[matches.Similarity < 0.001, \"To\"] = None\n",
    "        matches.loc[matches.Similarity < 0.001, \"Key\"] = None\n",
    "        for i in range(int(nbest) - 1):\n",
    "            matches.loc[matches.Similarity < 0.001, \"BestMatch\" + \"__\" + str(i+1)] = None\n",
    "            matches.loc[matches.Similarity < 0.001, \"Key\" + \"__\" + str(i+1)] = None\n",
    "        \n",
    "    return matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _create_ngrams(string: str) -> List[str]:\n",
    "    \"\"\" Create n_grams from a string\n",
    "\n",
    "     Steps:\n",
    "    * Extract character-level ngrams with `self.n_gram_range` (both ends inclusive)\n",
    "    * Remove n-grams that have a whitespace in them\n",
    "    \"\"\"\n",
    "    n_gram_range=(3, 3)\n",
    "    string = _clean_string(string)\n",
    "    result = []\n",
    "    for n in range(n_gram_range[0], n_gram_range[1]+1):\n",
    "        ngrams = zip(*[string[i:] for i in range(n)])\n",
    "        ngrams = [''.join(ngram) for ngram in ngrams if ' ' not in ngram]\n",
    "        result.extend(ngrams)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _clean_string(string: str) -> str:\n",
    "    \"\"\" Only keep alphanumerical characters and remove extra spaces \"\"\"\n",
    "    string = re.sub(r'[^A-Za-z0-9 ]+', '', string.lower())\n",
    "    string = re.sub('\\s+', ' ', string).strip()\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Fetch From database\n",
    "\n",
    "queryDF = pd.read_excel('./output/Stage1TopicDataHour.xlsx')#includes incident\n",
    "inputDF = pd.read_excel('./output/Stage2OutputDataHour.xlsx')#no incident"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import traceback\n",
    "import sys\n",
    "\n",
    "def connectDB(pdatabase, pUser, pPassword, pHost, pPort):\n",
    "    database = psycopg2.connect (database = pdatabase, user = pUser, password = pPassword, host = pHost, port = pPort)\n",
    "    return(database)\n",
    "\n",
    "\n",
    "def getInputDataFrame(pDatabase, pAsgCol, pAppCol, pTopicCol):\n",
    "    cursor = pDatabase.cursor()\n",
    "    try:\n",
    "        query = \"\"\"SELECT %s, %s, %s FROM historydata\"\"\"\n",
    "        cursor.execute(query,(pAsgCol, pAppCol, pTopicCol))\n",
    "        names = [pAsgCol, pAppCol, pTopicCol]\n",
    "        rows = cursor.fetchall()\n",
    "        pData = pd.DataFrame(rows, columns = names)\n",
    "        cursor.close()\n",
    "        return pData\n",
    "    except Exception as e:\n",
    "        cursor.close()\n",
    "        print('*** ERROR[0001]: getInputDataFrame: ', sys.exc_info()[0],str(e))\n",
    "        return(-1)\n",
    "    \n",
    "    \n",
    "def getQueryDataFrame(pDatabase, pAsgCol, pAppCol, pKeyCol, pDescCol):\n",
    "    cursor = pDatabase.cursor()\n",
    "    try:\n",
    "        query = \"\"\"SELECT %s, %s, %s, %s FROM ticket\"\"\"\n",
    "        cursor.execute(query,(pKeyCol, pDescCol, pAsgCol, pAppCol))\n",
    "        names = [pKeyCol, pDescCol, pAsgCol, pAppCol]\n",
    "        rows = cursor.fetchall()\n",
    "        pData = pd.DataFrame(rows, columns = names)\n",
    "        cursor.close()\n",
    "        return pData\n",
    "    except Exception as e:\n",
    "        cursor.close()\n",
    "        print('*** ERROR[0002]: getQueryDataFrame: ', sys.exc_info()[0],str(e))\n",
    "        return(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getRelevantTopics(pDatabaseConn, pKeyCol, pAsgCol, pAppCol, pDescCol, pTopicCol, nbest = 5):\n",
    "    try:\n",
    "        #Get the input parameters\n",
    "        pInputData = getInputDataFrame(pDatabaseConn, pAsgCol, pAppCol, pTopicCol):\n",
    "        appNames = pInputData[pAppCol].tolist()\n",
    "        topicNames = pInputData[pTopicCol].tolist()\n",
    "        asgNames = pInputData[pAsgCol].tolist()\n",
    "        listOfDataFrames = []\n",
    "        #get the query dataframe\n",
    "        pQueryData = getQueryDataFrame(pDatabase, pAsgCol, pAppCol, pKeyCol, pDescCol)\n",
    "        #Initialize the top-n tickets\n",
    "        nbest = int(nbest)\n",
    "        #Remove Duplicates and make combined pairs for efficient input\n",
    "        combinedList = [x+'|'+y+'|'+z for x, y, z in zip(asgNames, appNames, topicNames)]\n",
    "        combDict = {}\n",
    "        for item in combinedList:\n",
    "            combDict[item] = 1\n",
    "        for item in list(combDict.keys())[:]:\n",
    "            tempList = item.split('|')\n",
    "            inputAsgGroup = tempList[0]\n",
    "            inputAppName = tempList[1]\n",
    "            inputTopicName = tempList[2]\n",
    "            from_list = [inputTopicName]\n",
    "            to_list = pQueryData.groupby(pAsgCol).get_group(inputAsgGroup).groupby(pAppCol).get_group(inputAppName)[pDescCol].values.astype('U').tolist()\n",
    "            Train_CC = pQueryData.groupby(pAsgCol).get_group(inputAsgGroup).groupby(pAppCol).get_group(inputAppName)[pKeyCol].values.astype('U').tolist()\n",
    "            if len(to_list) < nbest:\n",
    "                continue\n",
    "            vectorizer = TfidfVectorizer(min_df=1, analyzer=_create_ngrams).fit(to_list + from_list)\n",
    "            X = vectorizer.transform(to_list)\n",
    "            Y = vectorizer.transform(from_list)\n",
    "            to_vector = csr_matrix(X)\n",
    "            from_vector = csr_matrix(Y)\n",
    "            matches = cosine_similarity(Y, X, from_list, to_list, Train_CC, nbest)\n",
    "            tempDF = pd.DataFrame()\n",
    "            tempDF[pDescCol] = matches.iloc[:,1:nbest+1].values.tolist()[-1]\n",
    "            tempDF[pKeyCol] = matches.iloc[:,nbest+1:2*nbest+1].values.tolist()[-1]\n",
    "            tempDF['similarity_index_score'] = [float(x) for x in matches.iloc[:,2*nbest+1:].values.tolist()[-1]]\n",
    "            tempDF[pAsgCol] = nbest*[inputAsgGroup]\n",
    "            tempDF[pAppCol] = nbest*[inputAppName]\n",
    "            tempDF[pTopicCol] = nbest*[inputTopicName]\n",
    "            listOfDataFrames.append(tempDF)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return (-1)\n",
    "    return (0,pd.concat(listOfDataFrames))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.98 s\n"
     ]
    }
   ],
   "source": [
    "%time _, resultdata = getRelevantTopics(queryDF, inputDF, 'number', 'assignment_group', 'cmdb_ci', 'description', 'Topic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description</th>\n",
       "      <th>number</th>\n",
       "      <th>similarity_index_score</th>\n",
       "      <th>assignment_group</th>\n",
       "      <th>cmdb_ci</th>\n",
       "      <th>Topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Testing was done on 08/11/20 by executing the ...</td>\n",
       "      <td>INC2899308</td>\n",
       "      <td>0.156889</td>\n",
       "      <td>CAP GRC Access Controls Application Support</td>\n",
       "      <td>SAP EVEREST GRC</td>\n",
       "      <td>access__enter__transaction__reopen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Cannot access into GRC Access Control, hence c...</td>\n",
       "      <td>INC1975095</td>\n",
       "      <td>0.148536</td>\n",
       "      <td>CAP GRC Access Controls Application Support</td>\n",
       "      <td>SAP EVEREST GRC</td>\n",
       "      <td>access__enter__transaction__reopen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>赤坂本社の古野です。 お疲れ様です。\\n\\n以下対応するようメッセージが何回か来ているのです...</td>\n",
       "      <td>INC1930612</td>\n",
       "      <td>0.145964</td>\n",
       "      <td>CAP GRC Access Controls Application Support</td>\n",
       "      <td>SAP EVEREST GRC</td>\n",
       "      <td>access__enter__transaction__reopen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>in GRC 12 -&gt; Reports and Analytics -&gt; Security...</td>\n",
       "      <td>INC2712216</td>\n",
       "      <td>0.143233</td>\n",
       "      <td>CAP GRC Access Controls Application Support</td>\n",
       "      <td>SAP EVEREST GRC</td>\n",
       "      <td>access__enter__transaction__reopen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reopen Incident: INC2007248-SOD History Report...</td>\n",
       "      <td>INC2048358</td>\n",
       "      <td>0.139086</td>\n",
       "      <td>CAP GRC Access Controls Application Support</td>\n",
       "      <td>SAP EVEREST GRC</td>\n",
       "      <td>access__enter__transaction__reopen</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         description      number  \\\n",
       "0  Testing was done on 08/11/20 by executing the ...  INC2899308   \n",
       "1  Cannot access into GRC Access Control, hence c...  INC1975095   \n",
       "2  赤坂本社の古野です。 お疲れ様です。\\n\\n以下対応するようメッセージが何回か来ているのです...  INC1930612   \n",
       "3  in GRC 12 -> Reports and Analytics -> Security...  INC2712216   \n",
       "4  Reopen Incident: INC2007248-SOD History Report...  INC2048358   \n",
       "\n",
       "   similarity_index_score                             assignment_group  \\\n",
       "0                0.156889  CAP GRC Access Controls Application Support   \n",
       "1                0.148536  CAP GRC Access Controls Application Support   \n",
       "2                0.145964  CAP GRC Access Controls Application Support   \n",
       "3                0.143233  CAP GRC Access Controls Application Support   \n",
       "4                0.139086  CAP GRC Access Controls Application Support   \n",
       "\n",
       "           cmdb_ci                               Topic  \n",
       "0  SAP EVEREST GRC  access__enter__transaction__reopen  \n",
       "1  SAP EVEREST GRC  access__enter__transaction__reopen  \n",
       "2  SAP EVEREST GRC  access__enter__transaction__reopen  \n",
       "3  SAP EVEREST GRC  access__enter__transaction__reopen  \n",
       "4  SAP EVEREST GRC  access__enter__transaction__reopen  "
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resultdata.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
